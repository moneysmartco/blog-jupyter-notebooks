{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the SEO Test Setup\n",
    "\n",
    "I.e. checking that the right urls are set up, without going too crazy on testing everything.\n",
    "\n",
    "In brief:\n",
    "\n",
    "* Focus on the articles\n",
    "* Check the test urls are accessible and show v2\n",
    "* If AMP is enabled, test that control urls show v1 AMP and test urls show v2\n",
    "* Check the canonicals\n",
    "* Check the breadcrumbs\n",
    "* That cloudflare looks to be working\n",
    "\n",
    "\n",
    "Out of scope:\n",
    "\n",
    "* That an AMP page is valid AMP (we have a separate test suite for this)\n",
    "* Being robust to html that isn't what's expected during blog v2 launch!\n",
    "* Testing non-article pages (home page, category page etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "import urllib\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random, randint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_test = \"sg\"\n",
    "test_staging = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_urls_csv_filepath = {  # These originally come from https://docs.google.com/spreadsheets/d/1UdrncwVdObaA6d0lnvbAQCHBpQMzEhWPz5OTR9Syobo/edit#gid=0\n",
    "    \"id\":\"./SEO Test Urls/id_seo_test_urls.csv\",\n",
    "    \"tw\":\"./SEO Test Urls/tw_seo_test_urls.csv\",\n",
    "    \"sg\": \"./SEO Test Urls/sg_seo_test_urls.csv\",\n",
    "    \"hk\": \"./SEO Test Urls/hk_seo_test_urls.csv\",\n",
    "}\n",
    "\n",
    "production_staging_urls = {\n",
    "    \"id\": [\"https://www.moneysmart.id\", \"https://www.msiddev.com\"],\n",
    "    \"tw\":[\"https://www.moneysmart.tw\", \"https://www.mstwdev.com\"],\n",
    "    \"sg\": [\"https://blog.moneysmart.sg\",\"https://blog.mssgdev.com\"],\n",
    "    \"hk\":[\"https://blog.moneysmart.hk\", \"https://blog.mshkdev.com\"],\n",
    "}\n",
    "\n",
    "def get_site_canonical_url():\n",
    "    \n",
    "    return production_staging_urls[country_to_test][(1 if test_staging else 0)]\n",
    "\n",
    "post_sitemaps = { #yes, we could load some more dynamically but it saves some coding\n",
    "    \"id\": [\"https://www.moneysmart.id/post-sitemap%i.xml\"%i for i in range(1,13)],\n",
    "    \"tw\": [\"https://www.moneysmart.tw/post-sitemap.xml\"],\n",
    "    \"sg\": [\"https://blog.moneysmart.sg/post-sitemap%i.xml\"%i for i in range(1,5)],\n",
    "    \"hk\": [\"https://blog.moneysmart.hk/post-sitemap%i.xml\"%i for i in range(1,3)],\n",
    "    \n",
    "    #Aside, there seems to be public pages for the ads in the sitemap :( \n",
    "    \n",
    "}\n",
    "\n",
    "# RSS feed requests are taken from scalyr by setting display to show uri, remote_ip, status\n",
    "# e.g. https://www.scalyr.com/events?filter=%22%2Ffeed%2F%22%20$status%3D200&teamToken=RsXhT5US%2FMZ7ugkOhT2g3g--&logSource=*blog-moneysmart-sg-*&showSystemPrefs=%5B%22timestamp%22%5D&showAdditionalFields=%5B%22uri%22,%22remote_ip%22,%22status%22%5D&startTime=7%20days&displayMode=%5Bobject%20Object%5D\n",
    "rss_feed_scalyr_requests_raw = {\n",
    "    \"id\":\"./RSS Feed Urls/id_scalyr_feed_requests_tidy.txt\",\n",
    "    \"tw\":\"./RSS Feed Urls/tw_scalyr_feed_requests_tidy.txt\",\n",
    "    \"sg\": \"./RSS Feed Urls/sg_scalyr_feed_requests_tidy.txt\",\n",
    "    \"hk\": \"./RSS Feed Urls/hk_scalyr_feed_requests_tidy.txt\",\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#Redirects\n",
    "\n",
    "#These are taken from Yoast.  You can go to yoast-> tools -> import and export to get a CSV of redirects.\n",
    "#https://admin.mstwdev.com/wp-admin/admin.php?page=wpseo_tools&tool=import-export#top#export-redirects\n",
    "#You could do a database query to a copy of production in future.\n",
    "\n",
    "staging_redirects_csvs = {\n",
    "    \"id\":\"./Redirects/id-staging-redirects.csv\",\n",
    "    \"tw\":\"./Redirects/tw-staging-redirects.csv\",\n",
    "    \"sg\":\"./Redirects/sg-staging-redirects.csv\",\n",
    "    \"hk\":\"./Redirects/hk-staging-redirects.csv\",\n",
    "    \n",
    "}\n",
    "\n",
    "production_redirects_csvs = {\n",
    "    \"id\":\"./Redirects/id-production-redirects.csv\",\n",
    "    \"tw\":\"./Redirects/tw-production-redirects.csv\",\n",
    "    \"sg\":\"./Redirects/sg-production-redirects.csv\",\n",
    "    \"hk\": \"./Redirects/hk-production-redirects.csv\",\n",
    "    \n",
    "}\n",
    "\n",
    "#These passwords aren't sensitive, they're just there to block search engine.\n",
    "v1_staging_passwords = {\n",
    "    \"id\":[\"admin\", \"moneysmart\"],\n",
    "    \"tw\":[\"admin\", \"Bubbl3\"],\n",
    "    \"sg\":[\"admin\", \"duri@n\"],\n",
    "    \"hk\":[\"admin\", \"yu3ch@\"],\n",
    "}\n",
    "\n",
    "v2_staging_passwords = {\n",
    "    \"id\":[\"admin\", \"moneysmart\"],\n",
    "    \"tw\":[\"admin\", \"moneysmart\"],\n",
    "    \"sg\":[\"admin\", \"moneysmart\"],\n",
    "    \"hk\":[\"admin\", \"moneysmart\"],\n",
    "}\n",
    "\n",
    "amp_is_enabled = {\n",
    "    \"id\":True,\n",
    "    \"tw\":False,\n",
    "    \"sg\":False,\n",
    "    \"hk\":False,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test URL Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping first line of url file: ['url']\n",
      "Found 50 test urls to check for\n"
     ]
    }
   ],
   "source": [
    "def load_seo_test_urls(country_code):\n",
    "    filepath = test_urls_csv_filepath[country_code]\n",
    "    urls = []\n",
    "    with open(filepath) as f:\n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i==0:\n",
    "                print(\"Skipping first line of url file: %s\"%row)\n",
    "                continue\n",
    "            urls.append(row[0].strip())\n",
    "\n",
    "    return urls\n",
    "\n",
    "seo_test_urls = load_seo_test_urls(country_to_test)\n",
    "\n",
    "print(\"Found %i test urls to check for\" % len(seo_test_urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blog.moneysmart.sg/entertainment/cheap-ktv-singapore-top-picks/\n",
      "https://blog.moneysmart.sg/dining/holland-village-food-price-guide/\n",
      "https://blog.moneysmart.sg/shopping/carousell-lazada-qoo10-shopee-seller/\n",
      "https://blog.moneysmart.sg/healthcare/cheap-dentists-singapore-dental-clinics/\n",
      "https://blog.moneysmart.sg/education/singapore-university-education-cost/\n",
      "https://blog.moneysmart.sg/shopping/amazon-prime-singapore/\n",
      "https://blog.moneysmart.sg/budgeting/iphone-xs-xr-price-singapore/\n",
      "https://blog.moneysmart.sg/budgeting/hair-salon-singapore-guide/\n",
      "https://blog.moneysmart.sg/travel/multi-currency-accounts-youtrip/\n",
      "https://blog.moneysmart.sg/healthcare/health-screening-singapore-cost/\n",
      "https://blog.moneysmart.sg/healthcare/acne-treatment-singapore-cost/\n",
      "https://blog.moneysmart.sg/budgeting/gst-voucher-u-save/\n",
      "https://blog.moneysmart.sg/property/rent-singapore-cost-guide/\n",
      "https://blog.moneysmart.sg/dining/bubble-tea-price-koi-liho-gongcha-taigai/\n",
      "https://blog.moneysmart.sg/invest/investing-shares-sgx-stocks-singapore/\n",
      "https://blog.moneysmart.sg/shopping/cheap-flower-delivery-singapore/\n",
      "https://blog.moneysmart.sg/shopping/muji-singapore-iuiga/\n",
      "https://blog.moneysmart.sg/credit-cards/best-debit-cards-singapore/\n",
      "https://blog.moneysmart.sg/invest/investing-gold-singapore/\n",
      "https://blog.moneysmart.sg/dining/ah-seng-durian-mao-shan-wang/\n",
      "https://blog.moneysmart.sg/invest/cdp-account-sgx/\n",
      "https://blog.moneysmart.sg/travel/expedia-promo-codes-singapore/\n",
      "https://blog.moneysmart.sg/shopping/don-don-donki-singapore/\n",
      "https://blog.moneysmart.sg/transportation/car-rental-singapore-guide/\n",
      "https://blog.moneysmart.sg/transportation/car-sharing-singapore-guide/\n",
      "https://blog.moneysmart.sg/invest/dbs-vickers-beginners-guide/\n",
      "https://blog.moneysmart.sg/fitness-beauty/true-fitness-first-virgin-active-gyms/\n",
      "https://blog.moneysmart.sg/transportation/traffic-fines-singapore-lta-ura-hdb-tp-erp/\n",
      "https://blog.moneysmart.sg/education/psle-tuition-cut-off-points/\n",
      "https://blog.moneysmart.sg/budgeting/cheap-haircut-singapore-barber-shop/\n",
      "https://blog.moneysmart.sg/dining/jewel-changi-airport-singapore/\n",
      "https://blog.moneysmart.sg/shopping/funan-mall-free-things/\n",
      "https://blog.moneysmart.sg/invest/sti-etf-guide-start-investing/\n",
      "https://blog.moneysmart.sg/life-insurance/life-insurance-singapore/\n",
      "https://blog.moneysmart.sg/healthcare/merdeka-generation/\n",
      "https://blog.moneysmart.sg/shopping/online-grocery-shopping-singapore-redmart-honestbee/\n",
      "https://blog.moneysmart.sg/dining/haidilao-hotpot-beauty-in-a-pot/\n",
      "https://blog.moneysmart.sg/budgeting/singtel-starhub-m1-iphone-11-prices/\n",
      "https://blog.moneysmart.sg/entertainment/cheap-chalets-singapore/\n",
      "https://blog.moneysmart.sg/property/hdb-bto-launch-may-2019/\n",
      "https://blog.moneysmart.sg/travel/5-countries-singaporeans-still-need-visa/\n",
      "https://blog.moneysmart.sg/credit-cards/best-air-miles-credit-cards-singapore/\n",
      "https://blog.moneysmart.sg/budgeting/cost-living-singapore/\n",
      "https://blog.moneysmart.sg/career/singapore-air-stewardess-cabin-crew-recruitment/\n",
      "https://blog.moneysmart.sg/budgeting/exchanging-foreign-currency-singapore-heres-way-decide-money-changer-visit/\n",
      "https://blog.moneysmart.sg/transportation/lta-rules-bikes-e-scooter-pmd/\n",
      "https://blog.moneysmart.sg/transportation/erp-rates-gantry-singapore/\n",
      "https://blog.moneysmart.sg/fitness-beauty/national-steps-challenge/\n",
      "https://blog.moneysmart.sg/transportation/ex-uber-drivers-drive-grab-ryde-taxi-companies/\n",
      "https://blog.moneysmart.sg/budgeting/aircon-servicing-singapore/\n"
     ]
    }
   ],
   "source": [
    "for a in seo_test_urls:print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check SEO Test URLs aren't Redirects\n",
    "\n",
    "Redirects implies that we won't be able to get sensible results from search console as they should have dropped off already to the redirected-to url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_redirects(country_code, for_staging, unencode_urls = False):\n",
    "    \"\"\"\n",
    "    Returns [from_url, to_url, redirect_code]\n",
    "    Where from_url is the full url (and the source CSV doesn't state as full url)\n",
    "    \n",
    "    Use unencode_urls for TW and HK\n",
    "    \"\"\"\n",
    "    print(\"cow\")\n",
    "    base_url = production_staging_urls[country_code][1 if for_staging else 0]\n",
    "    redirects_csv_path = None\n",
    "    if for_staging:\n",
    "        redirects_csv_path = staging_redirects_csvs[country_code]\n",
    "    else:\n",
    "        redirects_csv_path = production_redirects_csvs[country_code]\n",
    "\n",
    "    print(\"Loading all of the redirects from %s\" % redirects_csv_path)\n",
    "\n",
    "    redirects = [] #from, to, code\n",
    "    with open(redirects_csv_path) as f: #Origin,Target,Type,Format\n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i==0:\n",
    "                print(\"Skipping first line of url file: %s\"%row)\n",
    "                continue\n",
    "            from_url, to_url, redirect_code, redirect_format = row\n",
    "            if unencode_urls:\n",
    "                #TODO: not sure how safe this is unquoting, but doing to try to match TW urls.\n",
    "                from_url = urllib.parse.unquote(from_url)\n",
    "                to_url = urllib.parse.unquote(to_url)\n",
    "            from_url_full = urllib.parse.urljoin(base_url, from_url)+\"/\"\n",
    "            to_url_full = urllib.parse.urljoin(base_url, to_url)+\"/\"\n",
    "            redirects.append([from_url_full, to_url_full, redirect_code])\n",
    "\n",
    "    print(\"Loaded %i redirects\"% len(redirects))\n",
    "    return redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cow\n",
      "Loading all of the redirects from ./Redirects/sg-production-redirects.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Redirects/sg-production-redirects.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dce404b0db19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# (there might be a mismatch with staging, but this is the safest test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mproduction_redirects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_redirects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_to_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_staging\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munencode_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-662af144ff3e>\u001b[0m in \u001b[0;36mload_redirects\u001b[0;34m(country_code, for_staging, unencode_urls)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mredirects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#from, to, code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredirects_csv_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Origin,Target,Type,Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Redirects/sg-production-redirects.csv'"
     ]
    }
   ],
   "source": [
    "# Get the production redirects as at this point we're testing the production urls \n",
    "# (there might be a mismatch with staging, but this is the safest test)\n",
    "\n",
    "production_redirects = load_redirects(country_to_test, for_staging=False, unencode_urls = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_redirected_urls = [z[0] for z in production_redirects]\n",
    "seo_test_urls_that_are_redirects = [z for z in seo_test_urls if z in production_redirected_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the below two blocks look similar / the urls are likely to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_redirected_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seo_test_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(seo_test_urls_that_are_redirects)>0:\n",
    "    print(\"The following SEO test urls were redirects.  We should remove them from the test:\")\n",
    "    for a in seo_test_urls_that_are_redirects:\n",
    "        print(a)\n",
    "    \n",
    "else:\n",
    "    print(\"Looks like none of the SEO test urls are redirects, which is good.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_sitemap(sitemap_url, page_cache):\n",
    "    \n",
    "    #r = requests.get(sitemap_url)\n",
    "    #soup = BeautifulSoup(r.text, \"html.parser\") #html parser does xml fine\n",
    "    soup = page_cache.get_soup(sitemap_url)\n",
    "    urls = [element.text for element in soup.findAll('loc')]\n",
    "    return urls\n",
    "\n",
    "def get_all_post_urls_from_sitemap(country_code, page_cache):\n",
    "    # Note that separately I'm loading all the sitemaps, so you could use that instead if you can filter the posts.\n",
    "    sitemaps = post_sitemaps[country_code]\n",
    "    all_urls = []\n",
    "    for i, sitemap in enumerate(sitemaps):\n",
    "        #if test_staging:\n",
    "        #    sitemap = make_staging_url(sitemap)\n",
    "        try:\n",
    "            print(\"Loading urls from sitemap %i of %i %s\" % (i, len(sitemaps), sitemap))\n",
    "            sitemap_urls = get_urls_from_sitemap(sitemap, page_cache)\n",
    "            all_urls+=sitemap_urls\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: failed to load sitemap at %s with error %s\" %(sitemap, e))\n",
    "    return all_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sub_sitemaps_from_sitemap(sitemap_url, page_cache, recursive = False):\n",
    "    root_sitemap_soup = page_cache.get_soup(sitemap_url)\n",
    "    sub_sitemaps = []\n",
    "    for sitemap_block in root_sitemap_soup.find_all(\"sitemap\"):\n",
    "        sub_sitemap_url = sitemap_block.loc.text\n",
    "        sub_sitemaps.append(sub_sitemap_url)\n",
    "        if recursive:\n",
    "            next_levels =  extract_sub_sitemaps_from_sitemap(sub_sitemap_url, page_cache, recursive = True)\n",
    "            sub_sitemaps+=next_levels\n",
    "    return sub_sitemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_v2_url(url):\n",
    "    splits = urllib.parse.urlparse(url)\n",
    "    if \"www3.\" in splits.netloc or \"blog3.\" in splits.netloc:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_v1_url(url):\n",
    "    return not is_v2_url(url)\n",
    "\n",
    "    \n",
    "def make_staging_url(url):\n",
    "    # not responsible for auth\n",
    "    production_root, staging_root = production_staging_urls[country_to_test]\n",
    "    if production_root not in url:\n",
    "        raise Exception()\n",
    "        \n",
    "    staging_url = url.replace(production_root, staging_root)\n",
    "    \n",
    "    return staging_url\n",
    "    \n",
    "    \n",
    "def force_url_to_v1(url):\n",
    "    return url+\"?forceTest=control\"\n",
    "    \n",
    "def force_url_to_v2(url):\n",
    "    url_start, url_rest = url.split(\".\", 1)\n",
    "    if url_start == \"https://www\":\n",
    "        new_url = \"https://www3.\"+url_rest\n",
    "    elif url_start == \"https://blog\":\n",
    "        new_url = \"https://blog3.\"+url_rest\n",
    "    else:\n",
    "        new_url = url_start + \".\"+ url_rest\n",
    "    return new_url+\"?forceTest=test\"\n",
    "                                   \n",
    "\n",
    "def is_amp_page(url, page_cache):\n",
    "    \"\"\"\n",
    "    Very basic test on whether the page is marked in the html tag as AMP\n",
    "    \n",
    "       # <html ⚡> or <html amp>\n",
    "    rab\"\"\"\n",
    "    soup = page_cache.get_soup(url)\n",
    "    html_tag = soup.find('html')\n",
    "    html_tag.attrs\n",
    "    \n",
    "    if \"amp\" in html_tag.attrs or \"⚡\" in html_tag.attrs:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "def is_v2_page(url, page_cache):\n",
    "    \"\"\"\n",
    "    Very basic test if it's a v2 page based on knowledge of it.\n",
    "    \"\"\"\n",
    "    html = page_cache.get_html(url)\n",
    "    if \"nuxt\" in html:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "def is_v2_amp_page(url, page_cache):\n",
    "    if is_v2_page(url, page_cache) and is_amp_page(url, page_cache):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def extract_link_of_type(url, page_cache, type_of_link):\n",
    "    \"\"\"\n",
    "    Generally don't call this directly.\n",
    "    \n",
    "    Expects it to be unique in the page / no duplicates\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = page_cache.get_soup(url)\n",
    "    links = soup.find_all(\"link\")\n",
    "    suitable_link_tags = [z for z in links if \"rel\" in z.attrs and z.attrs[\"rel\"][0].lower()==type_of_link.lower()]\n",
    "    if len(suitable_link_tags)==0:\n",
    "        return None\n",
    "    if len(suitable_link_tags)>1:\n",
    "        raise Exception()\n",
    "    \n",
    "    link_url = suitable_link_tags[0].get(\"href\")\n",
    "    \n",
    "    return link_url\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def extract_link_to_amp_version(url, page_cache):\n",
    "    \"\"\"\n",
    "    Returns none if the amp version can't be found\n",
    "    \n",
    "    Looking for <link rel=\"amphtml\" href=\"...\"\n",
    "    \n",
    "    Returns the url it links to\n",
    "    \"\"\"\n",
    "    amp_url = extract_link_of_type(url, page_cache, \"amphtml\")\n",
    "    \n",
    "    return amp_url\n",
    "\n",
    "def amp_and_canonical_urls_match(amp_url, canonical_url):\n",
    "    if amp_url == canonical_url+\"?amp\": # better would be to do a join using url library\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def extract_canonical_tag(url, page_cache):\n",
    "    \"\"\"\n",
    "    \n",
    "    Looks for <link rel=\"canonical\" href=\"....\">\n",
    "    \n",
    "    This is similar to amphtml\n",
    "    \"\"\"\n",
    "    canonical_url = extract_link_of_type(url, page_cache, \"canonical\")\n",
    "    return canonical_url\n",
    "\n",
    "    \n",
    "def canonical_tag_is_valid(url, canonical_url):\n",
    "    \"\"\"\n",
    "    Returns bool, error_message\n",
    "    \n",
    "    \"\"\"\n",
    "    url_components = urllib.parse.urlparse(url)\n",
    "    canonical_components = urllib.parse.urlparse(canonical_url)\n",
    "    \n",
    "    error_message = \"\"\n",
    "    \n",
    "    \n",
    "    #Canonical uses https\n",
    "    if canonical_components.scheme != \"https\":\n",
    "        error_message += \"Canonical does't use https. \"\n",
    "    \n",
    "    # no query params in canonical\n",
    "    if canonical_components.query:\n",
    "        error_message += \"Canonical url contains query parameters. \"\n",
    "    \n",
    "    \n",
    "    # goes to base url\n",
    "    expected_root_url = get_site_canonical_url()\n",
    "    if not canonical_url.startswith(expected_root_url):\n",
    "        error_message += \"Canonical url doesn't appear to point to right site.\"\n",
    "    \n",
    "    \n",
    "    # url and canonical point to the same article\n",
    "    if url_components.path != canonical_components.path:\n",
    "        error_message += \"Canonical and url don't point to same url. \"\n",
    "    \n",
    "    \n",
    "    return error_message==\"\", error_message\n",
    "    \n",
    "def extract_meta_description(url, page_cache):\n",
    "    \"\"\"\n",
    "    <meta name=\"description\" content=\"Jika kamu...\">\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"getting meta description for %s\"%url)\n",
    "    soup = page_cache.get_soup(url)\n",
    "\n",
    "    \n",
    "    meta_tags = soup.find_all(\"meta\")\n",
    "    meta_tag_name = \"description\"\n",
    "    \n",
    "    suitable_tags = [z for z in meta_tags if \"name\" in z.attrs and z.attrs[\"name\"].lower()==meta_tag_name.lower()]\n",
    "    if len(suitable_tags)==0:\n",
    "        return None\n",
    "    if len(suitable_tags)>1:\n",
    "        raise Exception()\n",
    "    \n",
    "    #print(\"suitable tag %s\"%suitable_tags[0])\n",
    "    meta_description = suitable_tags[0].get(\"content\")\n",
    "    return meta_description\n",
    "    \n",
    "    \n",
    "def extract_ld_json_structured_data(url, page_cache, type_of_structured_data):\n",
    "    \"\"\"\n",
    "    Returns as parsed JSON i.e. should be able to inspect as a dictionary etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Note there's lots of valid ways of doing this, I'm just trying to match the way it's been implemented\n",
    "    \n",
    "    soup = page_cache.get_soup(url)\n",
    "    \n",
    "    script_tags = soup.find_all(\"script\")\n",
    "    sd_tags = [z for z in script_tags if \"type\" in z.attrs and z.attrs[\"type\"]==\"application/ld+json\"]\n",
    "    #print(sd_tags)\n",
    "    json_structured_datas = [json.loads(z.contents[0]) for z in sd_tags]\n",
    "    structured_datas_of_type = [z for z in json_structured_datas if z[\"@type\"]==type_of_structured_data]\n",
    "    if len(structured_datas_of_type) > 1:\n",
    "        raise Exception()\n",
    "    if len(structured_datas_of_type) == 0:\n",
    "        raise Exception() # just for debugging\n",
    "        return None\n",
    "    return structured_datas_of_type[0]\n",
    "    \n",
    "def extract_blogposting_structured_data(url, page_cache):\n",
    "    sd = extract_ld_json_structured_data(url, page_cache, \"BlogPosting\")\n",
    "    return sd\n",
    "\n",
    "\n",
    "def check_blogposting_structured_data_looks_sensible(url, page_cache):\n",
    "    \"\"\"\n",
    "    returns [bool, error_message]\n",
    "    \"\"\"\n",
    "    error_message = \"\"\n",
    "    sd = extract_blogposting_structured_data(url, page_cache)\n",
    "    \n",
    "    \n",
    "    expected_top_level_keys = [\"@type\", \n",
    "                               \"headline\", \n",
    "                               \"datePublished\", \n",
    "                               \"dateModified\",\n",
    "                               \"mainEntityOfPage\",\n",
    "                               \"image\",\n",
    "                               \"author\",\n",
    "                               \"publisher\",\n",
    "                              ]\n",
    "    missing_top_level_keys = [z for z in expected_top_level_keys if z not in sd]\n",
    "    if len(missing_top_level_keys)>0:\n",
    "        error_message += \"BlogPosting was missing the following keys: %s .  \"%missing_top_level_keys\n",
    "    \n",
    "    page_meta_description = extract_meta_description(url, page_cache)\n",
    "    bp_description = sd[\"description\"]\n",
    "    if bp_description != page_meta_description:\n",
    "        error_message+=\"Page meta description doesn't match blogposting description.  \\n %s \\n vs \\n %s\"%(page_meta_description,bp_description)\n",
    "        \n",
    "    #TODO: check headline\n",
    "    \n",
    "    logo_details = sd[\"publisher\"][\"logo\"]\n",
    "    logo_width = int(logo_details[\"width\"])\n",
    "    logo_height = int(logo_details[\"height\"])\n",
    "    \n",
    "    expected_logo_width = 447\n",
    "    expected_logo_height = 60\n",
    "    \n",
    "    if logo_width != expected_logo_width or logo_height!=expected_logo_height:\n",
    "        error_message+=\"Logo size in structured data was (%i, %i) not (%i, %i).  \"%(logo_width, logo_height, expected_logo_width, expected_logo_height)\n",
    "    \n",
    "    bp_id = sd[\"mainEntityOfPage\"][\"@id\"]\n",
    "    canonical_url = extract_canonical_tag(url, page_cache)\n",
    "    if bp_id != canonical_url:\n",
    "        error_message+=\"Expected the ID of the blogposting to be the canonical tag %s, but got %s.  \"% (canonical_url,bp_id)\n",
    "        \n",
    "    return (len(error_message)==0, error_message)\n",
    "        \n",
    "    \n",
    "    \n",
    "def extract_breadcrumbs_as_list(url, page_cache):\n",
    "    \"\"\"\n",
    "    Returns [[name, url]]\n",
    "    \n",
    "        {\n",
    "    \"@context\": \"https://schema.org\",\n",
    "    \"@type\": \"BreadcrumbList\",\n",
    "      \"itemListElement\": [{\n",
    "        \"@type\": \"ListItem\",\n",
    "        \"position\": 1,\n",
    "        \"name\": \"Books\",\n",
    "        \"item\": \"https://example.com/books\"\n",
    "      },\n",
    "      ...\n",
    "      }\n",
    "    \n",
    "    \"\"\"\n",
    "    sd = extract_ld_json_structured_data(url, page_cache, \"BreadcrumbList\")\n",
    "    \n",
    "    breadcrumbs = [[z[\"position\"], z[\"item\"][\"name\"], z[\"item\"][\"@id\"]] \\\n",
    "                   for z in sd[\"itemListElement\"]\\\n",
    "                      if z[\"@type\"]==\"ListItem\"\n",
    "                  ]\n",
    "    breadcrumbs.sort(key=lambda x: x[0]) #sort by position\n",
    "    \n",
    "    \n",
    "    return [z[1:] for z in breadcrumbs]\n",
    "\n",
    "\n",
    "def check_breadcrumbs_are_sensible(url, page_cache):\n",
    "    \"\"\"\n",
    "    Return (is_ok, error_message)\n",
    "    \"\"\"\n",
    "    # TODO: ideally would add some more checks on the validity of the structured data, but they can be done\n",
    "    # fairly easily with external tools.\n",
    "    \n",
    "    error_message = \"\"\n",
    "    breadcrumbs = extract_breadcrumbs_as_list(url, page_cache)\n",
    "    breadcrumb_names = [z[0] for z in breadcrumbs]\n",
    "    breadcrumb_urls = [z[1] for z in breadcrumbs]\n",
    "    \n",
    "    expected_url_root = get_site_canonical_url()\n",
    "    \n",
    "    \n",
    "    breadcrumbs_not_starting_with_root = [z for z in breadcrumb_urls if not z.startswith(expected_url_root)]\n",
    "    \n",
    "    if len(breadcrumbs_not_starting_with_root):\n",
    "        error_message+= \"Breadcrumbs didn't all start with the right url base (%s) %s . \"%(expected_url_root, breadcrumbs_not_starting_with_root)\n",
    "    \n",
    "    if breadcrumb_urls[0].strip(\"/\")!=expected_url_root:\n",
    "        error_message += \"Expected 1st breadcrumb to be homepage %s, but got %s.  \"%(expected_url_root, breadcrumb_urls[0])\n",
    "    \n",
    "    num_unique_urls =  len(set(breadcrumb_urls))\n",
    "    if num_unique_urls != len(breadcrumb_urls):\n",
    "        error_message += \"Breadcrumbs weren't unique.  Got %s .  \"%breadcrumb_urls\n",
    "    \n",
    "    #Check there's at least 3 (homepage, category, article)\n",
    "    \n",
    "    failing_category_urls = [z for z in breadcrumb_urls[1:-1] if \"/category/\" not in z]\n",
    "    if len(failing_category_urls)>0:\n",
    "        error_message += \"Category breadcrumbs don't look like category pages %s.  \"%failing_category_urls\n",
    "    \n",
    "    if len(breadcrumbs)<3:\n",
    "        error_message+=\"Expected at least homepage, category and article in breadcrumbs, but got %s.   \"%breadcrumbs\n",
    "    \n",
    "    return (len(error_message)==0, error_message)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageCache(object):\n",
    "    \"\"\"\n",
    "    This is an abstraction for getting a url, which understands\n",
    "    translation between staging and production, authentication etc\n",
    "    and uses a cache to allow efficient repeat requests.\n",
    "    (and whether that cache is in memory, or not is academic)\n",
    "    \n",
    "    It also includes throttling.\n",
    "    \n",
    "    The result might be a redirect to say www3 or whatever.\n",
    "    \n",
    "    NB this does currently rely on some globals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._page_cache_dict = {} #url:[page_html, load_time, cloudflare_in_headers, cloudflare_hit, url_after_redirect]\n",
    "        self._soup_cache_dict = {} #url: beautifulsoup\n",
    "        self._page_load_time_dict = {} # a log of how long it takes to load the pages in seconds\n",
    "        self.last_page_respone_time  = datetime.now()\n",
    "        self.min_seconds_between_server_requests = 0.5 #currently after request completion, rather than start\n",
    "    \n",
    "    def get_html(self, url, refresh_cache = False):\n",
    "        \"\"\"\n",
    "        This gets the url, but only requests it if it isn't in the page cache currently\n",
    "        \"\"\"\n",
    "        if refresh_cache or url not in self._page_cache_dict:\n",
    "            seconds_since_last_request = (datetime.now() - self.last_page_respone_time).total_seconds()\n",
    "            wait_time = self.min_seconds_between_server_requests - seconds_since_last_request\n",
    "            if wait_time>0:\n",
    "                print(\"Throttling request for %.2f seconds\"%wait_time)\n",
    "                time.sleep(wait_time)\n",
    "            \n",
    "            self._download_url_and_cache(url) # this also does things like check caching and do timing.\n",
    "            \n",
    "        html = self._page_cache_dict[url][0]\n",
    "        \n",
    "        self.last_page_request_time = datetime.now()\n",
    "        \n",
    "        return html\n",
    "        \n",
    "    def get_soup(self, url):\n",
    "        \"\"\"\n",
    "        Returns the beautifulsoup version of the page for tag extraction\n",
    "        \"\"\"\n",
    "        if url not in self._soup_cache_dict:\n",
    "            html = self.get_html(url)\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            self._soup_cache_dict[url] = soup\n",
    "\n",
    "        return self._soup_cache_dict[url]\n",
    "        \n",
    "\n",
    "        \n",
    "    def _download_url_and_cache(self, url):\n",
    "        user_pass = None\n",
    "        \n",
    "        \n",
    "        if test_staging:\n",
    "            #Need to deal with basic auth\n",
    "            # TODO\n",
    "            if is_v1_url(url):\n",
    "                user_pass = v1_staging_passwords[country_to_test]\n",
    "            else:\n",
    "                user_pass = v2_staging_passwords[country_to_test]\n",
    "            user_pass = tuple(user_pass) #required to make the requests library happy\n",
    "        \n",
    "        r = requests.get(url, auth= user_pass)\n",
    "        \n",
    "        if r.status_code == 401:\n",
    "            \n",
    "            #print(\"Got a 401 on %s with credentials %s\" % (url, user_pass))\n",
    "            if test_staging:\n",
    "                # print(\"Going to try again with other creds as likely redirect with different creds.\")\n",
    "                if not is_v1_url(url): # flipping counter-intuitively\n",
    "                    user_pass = v1_staging_passwords[country_to_test]\n",
    "                else:\n",
    "                    user_pass = v2_staging_passwords[country_to_test]\n",
    "                # print(\"Trying with creds %s\"%user_pass)\n",
    "                r = requests.get(url, auth=tuple(user_pass))\n",
    "                if r.status_code == \"401\":\n",
    "                    print(\"failed with other auth after trying two credentials\")\n",
    "                    raise Exception()\n",
    "            else:\n",
    "                raise Exception()\n",
    "        elif r.status_code!=200:\n",
    "            print(\"WARNING: STATUS CODE %s on %s\" % (str(r.status_code), url))\n",
    "        \n",
    "        r.raise_for_status() #Throw an exception if get e.g. a 404.\n",
    "        \n",
    "        load_time = r.elapsed.total_seconds()\n",
    "        text = r.text\n",
    "        headers = r.headers\n",
    "        \n",
    "        # Look at headers to judge if caching on the html is working\n",
    "        is_cdn_caching = False\n",
    "        is_cdn_cache_hit = False\n",
    "        if \"Server\" in headers:\n",
    "            if headers[\"Server\"]==\"cloudflare\":\n",
    "                is_cdn_caching = True\n",
    "                if headers[\"cf-cache-status\"]==\"HIT\":\n",
    "                    is_cdn_cache_hit = True\n",
    "            \n",
    "\n",
    "        url_after_redirect = r.url\n",
    "        #\"cf-cache-status\" \"HIT\"  /\"EXPIRED\"\n",
    "        #\"server:cloudflare\"\n",
    "        \n",
    "        self._page_cache_dict[url] = text, load_time, is_cdn_caching, is_cdn_cache_hit, url_after_redirect\n",
    "        \n",
    "    def page_was_cdn_cached_and_hit(self, url):\n",
    "        \"\"\"\n",
    "        Returns true if the last request to that url was cached.\n",
    "        \"\"\"\n",
    "        pc = self._page_cache_dict[url]\n",
    "        return pc[2], pc[3]\n",
    "        \n",
    "        \n",
    "    def page_response_time(self, url):\n",
    "        return self._page_cache_dict[url][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load page cache\n",
    "\n",
    "You can re-do this to clear all the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_cache = PageCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the tests of sg\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting the tests of %s\"%country_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Homepage still loads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the homepage loads https://blog.mssgdev.com\n",
      "Checking v1 on https://blog.mssgdev.com?forceTest=control\n",
      "Looks good\n",
      "Checking v2 on https://blog3.mssgdev.com?forceTest=test\n",
      "Looks good\n"
     ]
    }
   ],
   "source": [
    "homepage_url = production_staging_urls[country_to_test][(1 if test_staging else 0)]\n",
    "print(\"Checking that the homepage loads %s\"%homepage_url)\n",
    "\n",
    "for url, desc in [[force_url_to_v1(homepage_url), \"v1\"],[force_url_to_v2(homepage_url), \"v2\"]]:\n",
    "    print(\"Checking %s on %s\"%(desc, url))\n",
    "    html = page_cache.get_html(url)\n",
    "    if len(html)<1000:\n",
    "        raise Exception()\n",
    "    print(\"Looks good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Posts from Sitemaps (and test sitemap works!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading urls from sitemap 0 of 4 https://blog.moneysmart.sg/post-sitemap1.xml\n",
      "Loading urls from sitemap 1 of 4 https://blog.moneysmart.sg/post-sitemap2.xml\n",
      "Loading urls from sitemap 2 of 4 https://blog.moneysmart.sg/post-sitemap3.xml\n",
      "Loading urls from sitemap 3 of 4 https://blog.moneysmart.sg/post-sitemap4.xml\n",
      "Found 3238 posts in the sitemap\n",
      "e.g.\n",
      "\t https://blog.moneysmart.sg/home-loans/3-reasons-why-banks-are-rejecting-your-home-loan/\n",
      "\t https://blog.moneysmart.sg/opinion/singtel-wants-to-charge-you-for-whatsapp-screw-that-here-are-3-alternatives/\n",
      "\t https://blog.moneysmart.sg/invest/5-better-things-you-should-be-doing-with-your-ns-pay/\n",
      "\t https://blog.moneysmart.sg/transportation/5-cool-car-mods-thatll-result-in-lta-fining-or-imprisoning-your-ass/\n",
      "\t https://blog.moneysmart.sg/budgeting/heres-how-to-tell-if-that-old-comic-book-in-your-house-is-worth-a-lot-of-money/\n",
      "\n",
      "That leaves 3188 post urls that aren't in the test\n"
     ]
    }
   ],
   "source": [
    "# TODO: getting in a bit of a mess from here with staging vs production urls in different places\n",
    "all_post_urls = get_all_post_urls_from_sitemap(country_to_test, page_cache)\n",
    "\n",
    "print(\"Found %i posts in the sitemap\"%len(all_post_urls))\n",
    "print(\"e.g.\")\n",
    "for url in all_post_urls[:5]:\n",
    "    print(\"\\t %s\"%url)\n",
    "\n",
    "\n",
    "non_seo_test_urls = list(set(all_post_urls) - set(seo_test_urls))\n",
    "print()\n",
    "print(\"That leaves %i post urls that aren't in the test\" % len(non_seo_test_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://blog.moneysmart.sg/career/5-reasons-quit-job-even-though-like/',\n",
       " 'https://blog.moneysmart.sg/entertainment/halloween-horror-nights-zouk-halloween-2018/',\n",
       " 'https://blog.moneysmart.sg/life-insurance/whole-life-insurance-just-scam-read-first-decide/',\n",
       " 'https://blog.moneysmart.sg/budgeting/6-ways-waste-money-youre-office/',\n",
       " 'https://blog.moneysmart.sg/budgeting/wechat-pay-alipay/',\n",
       " 'https://blog.moneysmart.sg/career/corporate-bankruptcy-insolvency/',\n",
       " 'https://blog.moneysmart.sg/entertainment/the-3-worst-places-in-singapore-to-watch-a-movie/',\n",
       " 'https://blog.moneysmart.sg/wedding/3-biggest-things-singaporeans-focus-save-money-wedding/',\n",
       " 'https://blog.moneysmart.sg/home-loans/dont-pay-off-home-loan-refinance/',\n",
       " 'https://blog.moneysmart.sg/opinion/5-personality-traits-that-are-costing-singaporeans-money/',\n",
       " 'https://blog.moneysmart.sg/invest/singapore-savings-bonds-what-fixed-deposits-need-to-do-to-up-their-game/',\n",
       " 'https://blog.moneysmart.sg/dining/5-fine-dining-restaurants-can-dinner-less-100/',\n",
       " 'https://blog.moneysmart.sg/business/offline-vs-online-retail/',\n",
       " 'https://blog.moneysmart.sg/travel-insurance/best-travel-insurance-singapore/',\n",
       " 'https://blog.moneysmart.sg/travel/jb-shopping-malls/',\n",
       " 'https://blog.moneysmart.sg/life-insurance/life-stage-insurance/',\n",
       " 'https://blog.moneysmart.sg/travel/go-week-long-holiday-beijing-less-800/',\n",
       " 'https://blog.moneysmart.sg/travel/airbnb-japan-departure-sayonara-tax/',\n",
       " 'https://blog.moneysmart.sg/credit-cards/best-cashback-credit-cards-singapore-review/',\n",
       " 'https://blog.moneysmart.sg/transportation/gojek-grab-comfort-taxi-singapore/',\n",
       " 'https://blog.moneysmart.sg/career/how-much-do-hawkers-really-make/',\n",
       " 'https://blog.moneysmart.sg/education/5-things-must-youre-graduating-year/',\n",
       " 'https://blog.moneysmart.sg/opinion/3-ways-singaporeans-can-give-back-community-doesnt-involve-just-donating-money/',\n",
       " 'https://blog.moneysmart.sg/shopping/3-strategies-to-help-you-stop-buying-clothes-you-never-wear/',\n",
       " 'https://blog.moneysmart.sg/travel/enjoy-week-long-holiday-berlin-less-1600/',\n",
       " 'https://blog.moneysmart.sg/invest/dbs-digiportfolio-growing-money/',\n",
       " 'https://blog.moneysmart.sg/family/alternatives-school-bus-fees/',\n",
       " 'https://blog.moneysmart.sg/property/property-agent-commissions-singaporeans-might-not-know/',\n",
       " 'https://blog.moneysmart.sg/dining/starbucks-singapore-menu-prices/',\n",
       " 'https://blog.moneysmart.sg/wedding/7-ways-to-cut-your-wedding-venue-costs-in-singapore/',\n",
       " 'https://blog.moneysmart.sg/budgeting/m1-phone-plans-review/',\n",
       " 'https://blog.moneysmart.sg/health-insurance/international-health-insurance/',\n",
       " 'https://blog.moneysmart.sg/transportation/ban-e-scooter-pmd/',\n",
       " 'https://blog.moneysmart.sg/entertainment/3-hacks-that-can-lower-the-cost-of-dating-in-singapore/',\n",
       " 'https://blog.moneysmart.sg/career/singapore-expats-apply-singapore-visas/',\n",
       " 'https://blog.moneysmart.sg/family/4-things-singaporean-parents-can-do-to-help-their-kids-succeed/',\n",
       " 'https://blog.moneysmart.sg/home-loans/boc-home-loan-review/',\n",
       " 'https://blog.moneysmart.sg/entertainment/world-cup-2018/',\n",
       " 'https://blog.moneysmart.sg/credit-cards/3-best-cash-back-credit-cards-singapore/',\n",
       " 'https://blog.moneysmart.sg/credit-cards/best-ocbc-credit-cards-singapore-reviews/',\n",
       " 'https://blog.moneysmart.sg/shopping/winter-wear-singapore/',\n",
       " 'https://blog.moneysmart.sg/travel/4-cny-getaways-that-are-still-affordable-if-you-have-decided-on-taking-a-last-minute-trip/',\n",
       " 'https://blog.moneysmart.sg/family/4-things-mothers-in-singapore-should-do-to-plan-for-their-finances/',\n",
       " 'https://blog.moneysmart.sg/opinion/3-reasons-nobody-can-bothered-claim-free-early-morning-mrt-rides/',\n",
       " 'https://blog.moneysmart.sg/healthcare/health-screening-package/',\n",
       " 'https://blog.moneysmart.sg/credit-cards/best-credit-cards-weekend-dining/',\n",
       " 'https://blog.moneysmart.sg/property/5-things-singaporeans-wish-considered-buying-first-home/',\n",
       " 'https://blog.moneysmart.sg/career/companies-hire-older-pmets-not-just-fresh-grads-foreigners/',\n",
       " 'https://blog.moneysmart.sg/shopping/3-tricks-singaporeans-should-know-to-save-money-when-buying-groceries/',\n",
       " 'https://blog.moneysmart.sg/budgeting/electricity-retailer-singapore/']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_non_seo_test_sample_urls = 50\n",
    "non_seo_test_sample_urls = [non_seo_test_urls[randint(0, len(non_seo_test_urls)-1)] for z in range(num_non_seo_test_sample_urls)] # might not be unique, but whatever.\n",
    "non_seo_test_sample_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check all the Sitemaps Load Correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitemap_url = production_staging_urls[country_to_test][(1 if test_staging else 0)] +\"/sitemap.xml\"\n",
    "all_sitemaps =  [sitemap_url,] + extract_sub_sitemaps_from_sitemap( sitemap_url, page_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 sitemaps in all\n"
     ]
    }
   ],
   "source": [
    "print(\"Found %i sitemaps in all\" % len(all_sitemaps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sitemap https://blog.mssgdev.com/sitemap.xml had 23 urls within it\n",
      "Sitemap https://blog.mssgdev.com/post-sitemap1.xml had 1000 urls within it\n",
      "Sitemap https://blog.mssgdev.com/post-sitemap2.xml had 1000 urls within it\n",
      "Sitemap https://blog.mssgdev.com/post-sitemap3.xml had 1000 urls within it\n",
      "Sitemap https://blog.mssgdev.com/post-sitemap4.xml had 261 urls within it\n",
      "Sitemap https://blog.mssgdev.com/page-sitemap.xml had 7 urls within it\n",
      "Sitemap https://blog.mssgdev.com/attachment-sitemap1.xml had 1000 urls within it\n",
      "Sitemap https://blog.mssgdev.com/attachment-sitemap2.xml had 1000 urls within it\n",
      "Sitemap https://blog.mssgdev.com/attachment-sitemap3.xml had 1000 urls within it\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://blog3.mssgdev.com/attachment-sitemap4.xml",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e0f20ae4bb06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msitemap\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_sitemaps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0murls_in_sitemap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_urls_from_sitemap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msitemap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this includes sub-sitemaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mall_urls_in_sitemaps\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0murls_in_sitemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sitemap %s had %i urls within it\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msitemap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls_in_sitemap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e7dfd55d869b>\u001b[0m in \u001b[0;36mget_urls_from_sitemap\u001b[0;34m(sitemap_url, page_cache)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#r = requests.get(sitemap_url)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#soup = BeautifulSoup(r.text, \"html.parser\") #html parser does xml fine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msitemap_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-903291612c73>\u001b[0m in \u001b[0;36mget_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_soup_cache_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_soup_cache_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-903291612c73>\u001b[0m in \u001b[0;36mget_html\u001b[0;34m(self, url, refresh_cache)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_url_and_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this also does things like check caching and do timing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_page_cache_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-903291612c73>\u001b[0m in \u001b[0;36m_download_url_and_cache\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARNING: STATUS CODE %s on %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Throw an exception if get e.g. a 404.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mload_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://blog3.mssgdev.com/attachment-sitemap4.xml"
     ]
    }
   ],
   "source": [
    "all_urls_in_sitemaps = []\n",
    "\n",
    "for sitemap in all_sitemaps:\n",
    "    \n",
    "    urls_in_sitemap = get_urls_from_sitemap(sitemap, page_cache) # this includes sub-sitemaps\n",
    "    all_urls_in_sitemaps+= urls_in_sitemap\n",
    "    print(\"Sitemap %s had %i urls within it\"%(sitemap, len(urls_in_sitemap)))\n",
    "    if len(urls_in_sitemap)==0:\n",
    "        raise Exception()\n",
    "        \n",
    "num_urls_from_sitemap = len(all_urls_in_sitemaps)\n",
    "num_distinct_urls_from_sitemap = len(set(all_urls_in_sitemaps))\n",
    "\n",
    "if num_urls_from_sitemap != num_distinct_urls_from_sitemap:\n",
    "    print(\"ERROR: not all urls in sitemap were distinct\")\n",
    "    print(\"%i in the sitemap, %i distinct\"%(num_urls_from_sitemap, num_distinct_urls_from_sitemap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the AB split percentage (approximately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get url in the right format for the environment\n",
    "requests_to_check_ab_split = 100\n",
    "test_url = non_seo_test_urls[randint(0, len(non_seo_test_urls))]\n",
    "hit_v2s = []\n",
    "print(\"Testing %i requests outside of SEO test to see AB split with url %s\" % (requests_to_check_ab_split, test_url))\n",
    "for i in range(requests_to_check_ab_split):\n",
    "    if i%5==0:\n",
    "        print(\"Testing %i / %i\"%(i, requests_to_check_ab_split))\n",
    "    html=page_cache.get_html(test_url, refresh_cache = True)\n",
    "    is_v2 = is_v2_page(test_url, page_cache)\n",
    "    hit_v2s.append(is_v2)\n",
    "    \n",
    "num_v1s = len([z for z in hit_v2s if z==False])\n",
    "num_v2s = len([z for z in hit_v2s if z==True])\n",
    "percentage_v2 = 100 * float(num_v2s)/ (num_v1s + num_v2s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"v1\", \"v2 - %.1f\" % percentage_v2]\n",
    "sizes = [int(z) for z in [num_v1s, num_v2s]]\n",
    "    \n",
    "plt.pie(sizes, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check SEO Test Urls Exist on Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_pass_seo_test = [] #\n",
    "for url in seo_test_urls:\n",
    "    html =page_cache.get_html(url, refresh_cache=True)\n",
    "    if len(html)<2000:\n",
    "        print(\"FAIL %s as page looks too small\"%url)\n",
    "    else:\n",
    "        print(\"pass %s\"%url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check SEO Test Articles v2 Loads (for before test is setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in seo_test_urls:\n",
    "    if test_staging:\n",
    "        url = make_staging_url(url)\n",
    "    url = force_url_to_v2(url)\n",
    "    print(\"Testing %s\" % url)\n",
    "    if not is_v2_page(url, page_cache):\n",
    "        print(\"ERROR: %s isn't v2\" %url)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check SEO Test Articles are v2 (for once test is setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_thoroughly = True # does a lot more page requests\n",
    "repeat_per_url_times = 2\n",
    "urls_pass_seo_test = [] #\n",
    "for url in seo_test_urls:\n",
    "    if test_staging:\n",
    "        url = make_staging_url(url)\n",
    "    passes = False\n",
    "    try:\n",
    "        if do_thoroughly:\n",
    "            for a in range(repeat_per_url_times):\n",
    "                page_cache.get_html(url, refresh_cache=True)\n",
    "                is_v2 = is_v2_page(url,page_cache)\n",
    "                if is_v2: \n",
    "                    passes=True\n",
    "                    break\n",
    "        else:\n",
    "            passes = is_v2_page(url, page_cache)\n",
    "    except Exception as e:\n",
    "        print(\"%s is TOTAL FAIL with exception %s\"%(url, str(e)))\n",
    "    print(\"%s %s\"%(url, (\"pass\" if passes else \"FAIL\")))\n",
    "    urls_pass_seo_test.append(passes)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"All pages pass: %s\"%str(all(urls_pass_seo_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_seo_test_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy copy / paste with a bit of modification\n",
    "\n",
    "print(\"Testing a set of urls that aren't in the SEO test point to v1 (at least sometimes)\")\n",
    "approx_num_test_urls = 3\n",
    "non_seo_test_urls_to_test = [non_seo_test_urls[randint(0, len(non_seo_test_urls))] for z in range(approx_num_test_urls)]\n",
    "non_seo_test_urls_to_test = non_seo_test_urls[:5] #TODO: REMOVE\n",
    "do_thoroughly = True # does a lot more page requests\n",
    "repeat_per_url_times = 2\n",
    "urls_pass_seo_test = [] #\n",
    "for url in non_seo_test_urls_to_test:\n",
    "    if test_staging:\n",
    "        url = make_staging_url(url)\n",
    "    passes = False\n",
    "    try:\n",
    "        if do_thoroughly:\n",
    "            for a in range(repeat_per_url_times):\n",
    "                page_cache.get_html(url, refresh_cache=True)\n",
    "                is_v1 = not(is_v2_page(url,page_cache))\n",
    "                if is_v1: \n",
    "                    passes=True\n",
    "                    break\n",
    "        else:\n",
    "            passes = not(is_v2_page(url, page_cache))\n",
    "    except Exception as e:\n",
    "        print(\"%s is TOTAL FAIL with exception %s\"%(url, str(e)))\n",
    "        #raise e\n",
    "    print(\"%s %s\"%(url, (\"pass\" if passes else \"FAIL\")))\n",
    "    urls_pass_seo_test.append(passes)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"All pages pass: %s\"%str(all(urls_pass_seo_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Check Test Articles have v2 AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if amp_is_enabled[country_to_test]:\n",
    "    for url in seo_test_urls:\n",
    "\n",
    "        if test_staging:\n",
    "            url = make_staging_url(url)\n",
    "        print(\"Testing %s\"%url)\n",
    "        amp_url = extract_link_to_amp_version(url, page_cache)\n",
    "        if not amp_url:\n",
    "            raise Exception()\n",
    "\n",
    "        amp_url_is_valid = is_v2_amp_page(amp_url, page_cache)\n",
    "        if not amp_url_is_valid:\n",
    "            raise Exception()\n",
    "\n",
    "        print(\"Looks good.\")\n",
    "else:\n",
    "    print(\"Skipping test as country doesn't have AMP enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Check Canonical Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_canonical_check_to_v2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site_canonical_url = get_site_canonical_url()\n",
    "# print(\"Checking canonical tags against %s\"%site_canonical_url)\n",
    "for url in seo_test_urls + non_seo_test_sample_urls:\n",
    "    \n",
    "    if test_staging:\n",
    "        url = make_staging_url(url)\n",
    "    #print(\"Testing %s \"% url)\n",
    "    \n",
    "    if force_canonical_check_to_v2:\n",
    "        url = force_url_to_v2(url)\n",
    "    try:\n",
    "        canonical_url = extract_canonical_tag(url, page_cache)\n",
    "        print(\"%s \\n\\thas canonical %s\"%(url, canonical_url))\n",
    "        is_valid, error_message = canonical_tag_is_valid(url, canonical_url)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: failed to load %s with exception %s\"%(url, e))\n",
    "    \n",
    "    if not is_valid: \n",
    "        #raise Exception()\n",
    "        print(\"ERROR: \"+ error_message+ \"\\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Check Test Articles and Non-Test Articles Have Sensible Structured Data\n",
    "\n",
    "I.e.\n",
    "\n",
    "* Use the canonical url\n",
    "* Articles have 3 or 4 levels (home page, category, article) in the breadcrumbs\n",
    "* BlogPosting at least exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check BlogPosting Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in seo_test_urls+ non_seo_test_sample_urls:\n",
    "    if test_staging:\n",
    "        url = make_staging_url(url)\n",
    "        \n",
    "    url = force_url_to_v2(url)\n",
    "    print()\n",
    "    print(\"Testing %s\"%url)\n",
    "    try:\n",
    "        is_good, error_message = check_blogposting_structured_data_looks_sensible(url, page_cache)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: %s failed with message %s\" % (url, e))\n",
    "    if not is_good:\n",
    "        print(\"ERROR: \"+ error_message)\n",
    "        #raise Exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check The Breadcrumbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in seo_test_urls + non_seo_test_sample_urls:\n",
    "    if test_staging:\n",
    "        url = make_staging_url(url)\n",
    "    url = force_url_to_v2(url)\n",
    "    print()\n",
    "    print(\"Testing %s\"%url)\n",
    "    is_good, error_message = check_breadcrumbs_are_sensible(url, page_cache)\n",
    "    if not is_good:\n",
    "        print(\"ERROR: \"+ error_message)\n",
    "        #raise Exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Cloudflare is (to some extent) Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that v2 is cached\n",
    "cache_test_article = non_seo_test_sample_urls[0]\n",
    "if test_staging:\n",
    "    cache_test_article = make_staging_url(cache_test_article)\n",
    "cache_test_article = force_url_to_v2(cache_test_article)\n",
    "\n",
    "\n",
    "for a in range(3):\n",
    "    page_cache.get_html(cache_test_article, refresh_cache=True)\n",
    "is_cached, cache_hit = page_cache.page_was_cdn_cached_and_hit(cache_test_article)\n",
    "page_load_time = page_cache.page_response_time(cache_test_article)\n",
    "print(\"Requested v2 url %s in\"%(cache_test_article,))\n",
    "print(\"Request took  %.3f\"% (page_load_time,))\n",
    "print(\"Cached (cloudflare runs) = %s\"%is_cached)\n",
    "print(\"Cache hit (actually cached) = %s\"%cache_hit)\n",
    "\n",
    "if not cache_hit or page_load_time>0.05:\n",
    "    print(\"ERROR: doesn't look properly cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: >> this is giving too quick response somehow.\n",
    "\n",
    "# Check v1 caching (if SG or HK, not expected to be cached)\n",
    "cache_test_article = non_seo_test_sample_urls[0]\n",
    "if test_staging:\n",
    "    cache_test_article = make_staging_url(cache_test_article)\n",
    "cache_test_article = force_url_to_v1(cache_test_article)\n",
    "\n",
    "\n",
    "for a in range(3):\n",
    "    page_cache.get_html(cache_test_article, refresh_cache=True)\n",
    "is_cached, cache_hit = page_cache.page_was_cdn_cached_and_hit(cache_test_article)\n",
    "page_load_time = page_cache.page_response_time(cache_test_article)\n",
    "\n",
    "    \n",
    "print(\"Requested v1 url %s in\"%(cache_test_article,))\n",
    "print(\"Request took  %.3f\"% (page_load_time,))\n",
    "print(\"Cached (cloudflare runs) = %s\"%is_cached)\n",
    "print(\"Cache hit (actually cached) = %s\"%cache_hit)\n",
    "print(country_to_test)\n",
    "if country_to_test.lower() in [\"sg\", \"hk\"]:\n",
    "    if cache_hit:\n",
    "        print(\"ERROR, v1 shouldn't be cached\")\n",
    "    else:\n",
    "        print(\"pass\")\n",
    "else:\n",
    "    if not cache_hit:\n",
    "        print(\"ERROR, v2 should be cached\")\n",
    "    elif page_load_time>0.05:\n",
    "        print(\"ERROR, page load time was longer than expected for a cache hit\")\n",
    "    else:\n",
    "        print(\"pass\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check That Article Redirects are Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: ideally check the redirect status, but not really sure how to do this nicely (it is in request.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects = load_redirects(country_to_test, for_staging = test_staging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_only_sample_of_redirects = True\n",
    "\n",
    "num_redirects_to_test_if_only_sample = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_sample_of_redirects:\n",
    "    redirects_to_test = redirects[:num_redirects_to_test_if_only_sample] #better would be random sampling\n",
    "else:\n",
    "    redirects_to_test = redirects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redirects on v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for from_url, to_url, status_code in redirects_to_test:\n",
    "    from_url = force_url_to_v1(from_url)\n",
    "    try:\n",
    "        \n",
    "        html = page_cache.get_html(from_url)\n",
    "    except:\n",
    "        html = \"\"\n",
    "    if len(html)<2000:\n",
    "        print(\"\\nFAIL %s \\n\"% from_url)\n",
    "    else:\n",
    "        print(\"pass %s\", from_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redirects on v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple test based on if the page loads and there's some sensible html length\n",
    "#TODO: actually check destination page\n",
    "for from_url, to_url, status_code in redirects_to_test:\n",
    "    from_url = force_url_to_v2(from_url)\n",
    "    try:\n",
    "        \n",
    "        html = page_cache.get_html(from_url)\n",
    "    except:\n",
    "        html = \"\"\n",
    "    if len(html)<2000:\n",
    "        print(\"\\nFAIL %s \\n\"% from_url)\n",
    "    else:\n",
    "        print(\"pass %s\"% from_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check RSS Feeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the core RSS feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_rss_url = urllib.parse.urljoin(get_site_canonical_url(), \"/feed/\")\n",
    "core_rss_body = page_cache.get_html(core_rss_url)\n",
    "if \"xml\" in core_rss_body:\n",
    "    print(\"Basic RSS feed looks good\")\n",
    "else:\n",
    "    print(\"FAIL: RSS feed at %s doesn't look good\"%core_rss_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: should really run these across more RSS feeds\n",
    "\n",
    "if \"www3\" in core_rss_body:\n",
    "    print(\"ERROR: www3 in rss feed\")\n",
    "    \n",
    "if \"blog3\" in core_rss_body:\n",
    "    print(\"ERROR: blog3 in rss feed\")\n",
    "    \n",
    "if \"admin.\" in core_rss_body or \"blog-admin.\" in core_rss_body:\n",
    "    print(\"ERROR: looks like an admin url in the rss feed somewhere.\")\n",
    "    \n",
    "if \"api.\" in core_rss_body:\n",
    "    print(\"ERROR: looks like an API url in the rss feed somewhere.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Full List of RSS feed (a lot of requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rss_feed_urls_from_scalyr_logs(scalyr_log_file_path):\n",
    "    \"\"\"\n",
    "    Expects a file as per the result of https://www.scalyr.com/events?filter=%22%2Ffeed%2F%22%20$status%3D200&teamToken=RsXhT5US%2FMZ7ugkOhT2g3g--&logSource=*blog-moneysmart-sg-*&showSystemPrefs=%5B%22timestamp%22%5D&showAdditionalFields=%5B%22uri%22,%22remote_ip%22,%22status%22%5D&startTime=7%20days&displayMode=%5Bobject%20Object%5D\n",
    "    i.e. lines like:  00:18:12.000 uri='/comments/feed/' remote_ip='62.210.215.109' status=200\n",
    "    \n",
    "    Returns localised to staging / production\n",
    "    \"\"\"\n",
    "    base_url = get_site_canonical_url()\n",
    "    with open(scalyr_log_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        print(\"%i lines\"%len(lines))\n",
    "        urls = [z.split(\" uri=\")[1].split(\" \")[0].strip(\"'\") for z in lines]\n",
    "        urls_filtered = [z for z in urls if \"/feed/\" in z]\n",
    "    \n",
    "    distinct_urls = list(set(urls_filtered))\n",
    "    distinct_urls.sort()\n",
    "    full_distinct_urls = [urllib.parse.urljoin(base_url, z) for z in distinct_urls]\n",
    "    return full_distinct_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_log_urls = rss_feed_scalyr_requests_raw[country_to_test]\n",
    "rss_urls = load_rss_feed_urls_from_scalyr_logs(rss_log_urls)\n",
    "print(\"found %i RSS feed urls \"% len(rss_urls))\n",
    "print(\"e.g. (first 10):\")\n",
    "for a in rss_urls[:10]:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passing_rss_feeds = []\n",
    "failing_rss_feeds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rss_urls_to_check = 200 # put -1 for all \n",
    "\n",
    "print(\"Checking the first %i of the rss feed urls\"%num_rss_urls_to_check)\n",
    "num_rss_urls = len(rss_urls)\n",
    "for i, rss_url in enumerate(rss_urls[:num_rss_urls_to_check]):\n",
    "    if i<10:\n",
    "        print(\"checking %i %s\"%(i, rss_url))\n",
    "    if i%10==0:\n",
    "        print(\"Checking %i / %i\"%(i, num_rss_urls))\n",
    "    try:\n",
    "        rss_text = page_cache.get_html(rss_url)\n",
    "        if \"xml\" in rss_text:\n",
    "            passing_rss_feeds.append(rss_url)\n",
    "        else:\n",
    "            failing_rss_feeds.append(rss_url)\n",
    "    except:\n",
    "        failing_rss_feeds.append(rss_url)\n",
    "        \n",
    "print(\"Got %i passing and %i failing RSS feeds\"%(len(passing_rss_feeds), len(failing_rss_feeds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(failing_rss_feeds)>0:\n",
    "    failing_rss_feeds_to_focus = [z for z in failing_rss_feeds if urllib.parse.urlsplit(z).path.startswith(\"/feed\")]\n",
    "    print(\"First 20 urls to focus on:\")\n",
    "    for a in failing_rss_feeds_to_focus[:20]:\n",
    "        print(a)\n",
    "    print(\"\")\n",
    "    print(\"First 20 failing rss urls overall:\")\n",
    "    for a in failing_rss_feeds[:20]:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
